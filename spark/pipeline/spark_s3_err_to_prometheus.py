

import time
from datetime import datetime, timedelta
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, from_json, row_number, lit, to_timestamp, date_format
from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, BooleanType
from prometheus_client import start_http_server, Gauge

# --- ì„¤ì • ---
PROMETHEUS_PORT = 9991  # ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì™€ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ìƒˆ í¬íŠ¸
REFRESH_INTERVAL_SECONDS = 30
DATA_RETENTION_MINUTES = 60  # 60ë¶„ ì´ë‚´ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©

# --- í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì •ì˜ ---
METRIC_LABELS = ['device_id']
ERR_METRICS = {
    'sensor1': Gauge('fms_err_latest_sensor1', 'Latest err value of sensor1', METRIC_LABELS),
    'sensor2': Gauge('fms_err_latest_sensor2', 'Latest err value of sensor2', METRIC_LABELS),
    'sensor3': Gauge('fms_err_latest_sensor3', 'Latest err value of sensor3', METRIC_LABELS),
    'motor1': Gauge('fms_err_latest_motor1', 'Latest err value of motor1', METRIC_LABELS),
    'motor2': Gauge('fms_err_latest_motor2', 'Latest err value of motor2', METRIC_LABELS),
    'motor3': Gauge('fms_err_latest_motor3', 'Latest err value of motor3', METRIC_LABELS),
}

# --- JSON ë°ì´í„° ìŠ¤í‚¤ë§ˆ ---
JSON_SCHEMA = StructType() \
    .add("time", StringType()) \
    .add("DeviceId", IntegerType()) \
    .add("sensor1", DoubleType()) \
    .add("sensor2", DoubleType()) \
    .add("sensor3", DoubleType()) \
    .add("motor1", DoubleType()) \
    .add("motor2", DoubleType()) \
    .add("motor3", DoubleType()) \
    .add("isFail", BooleanType()) \
    .add("collected_at", StringType())

def update_err_metrics(spark):
    now = datetime.now()
    partition_path = f"s3a://awsprelab1/fms/analytics_parquet/dataerr/{now.year:04d}/{now.month:02d}/{now.day:02d}/{now.hour:02d}"
    """S3ì˜ err JSON ë°ì´í„°ë¥¼ ì½ê³ , ì¥ë¹„ë³„ ìµœì‹  ê°’ì„ ì°¾ì•„ í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ ìµœì‹  'err' ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤... (ê²½ë¡œ: {partition_path})")
    
    try:
        # 1. S3 ê²½ë¡œì˜ JSON íŒŒì¼ ì½ê¸°
        # 1. íŒŒí‹°ì…”ë‹ëœ ê²½ë¡œë§Œ ì½ê¸°
        df_err = spark.read.text(partition_path)
        
        # 2. JSON íŒŒì‹± ë° ì»¬ëŸ¼ ì¶”ì¶œ
        df_parsed = df_err.select(from_json(col("value"), JSON_SCHEMA).alias("data")).select("data.*")

        # 3. íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± ë° 60ë¶„ ì´ë‚´ ë°ì´í„° í•„í„°ë§
        df_with_ts = df_parsed.withColumn("ts", to_timestamp(col("collected_at")))
        time_threshold = datetime.now() - timedelta(minutes=DATA_RETENTION_MINUTES)
        df_recent = df_with_ts.filter(col("ts") >= lit(time_threshold).cast("timestamp"))

        if df_recent.rdd.isEmpty():
            print("âœ… ì²˜ë¦¬í•  ìµœì‹  'err' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for gauge in ERR_METRICS.values():
                gauge.clear()
            return

        # 4. ê° DeviceId ë‚´ì—ì„œ time ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ë ˆì½”ë“œ ì°¾ê¸°
        window_spec = Window.partitionBy("DeviceId").orderBy(col("time").desc())
        df_latest = df_recent.withColumn("rank", row_number().over(window_spec)) \
                             .filter(col("rank") == 1) \
                             .select("DeviceId", "sensor1", "sensor2", "sensor3", "motor1", "motor2", "motor3")

        # 5. ì°¾ì€ ìµœì‹  ê°’ì„ í”„ë¡œë©”í…Œìš°ìŠ¤ ê²Œì´ì§€ì— ë°˜ì˜
        for gauge in ERR_METRICS.values():
            gauge.clear()
            
        for row in df_latest.collect():
            labels = {"device_id": str(row["DeviceId"])}
            for metric_name, gauge in ERR_METRICS.items():
                value = row[metric_name]
                if value is not None:
                    gauge.labels(**labels).set(float(value))
        
        print(f"âœ… {df_latest.count()}ê°œ 'err' ì¥ë¹„ì˜ ìµœì‹  ë©”íŠ¸ë¦­ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        if "Path does not exist" in str(e):
            print(f"âš ï¸ S3 ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {partition_path}. 'err' ë°ì´í„°ê°€ ì•„ì§ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜: Spark ì„¸ì…˜ ìƒì„±, í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ ì‹œì‘ ë° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ë£¨í”„ ì‹¤í–‰"""
    spark = SparkSession.builder \
        .appName("FMS S3 Err to Prometheus") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")

    print(f"ğŸš€ 'err' ë°ì´í„° í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (í¬íŠ¸: {PROMETHEUS_PORT})")
    start_http_server(PROMETHEUS_PORT)
    print(f"ğŸ”— ë©”íŠ¸ë¦­ í™•ì¸: http://<your-spark-driver-ip>:{PROMETHEUS_PORT}")

    try:
        while True:
            update_err_metrics(spark)
            print(f"ğŸ•’ ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ {REFRESH_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            time.sleep(REFRESH_INTERVAL_SECONDS)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
