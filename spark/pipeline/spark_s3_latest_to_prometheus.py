

import time
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, row_number, current_timestamp, expr
from prometheus_client import start_http_server, Gauge

# --- ì„¤ì • ---
S3_DATA_PATH = "s3a://awsprelab1/fms/analytics_parquet/data"
PROMETHEUS_PORT = 9990  # ì´ì „ ìŠ¤í¬ë¦½íŠ¸ì™€ ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
REFRESH_INTERVAL_SECONDS = 10  # 10ì´ˆë§ˆë‹¤ ë°ì´í„° ê°±ì‹ 

# --- í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì •ì˜ ---
# 'device_id'ë¥¼ ë¼ë²¨ë¡œ ì‚¬ìš©í•˜ì—¬ ì¥ë¹„ë³„ ìµœì‹  ê°’ì„ êµ¬ë¶„
LATEST_METRICS = {
    'latest_sensor1': Gauge('fms_latest_sensor1', 'Latest value of sensor1 for a device', ['device_id']),
    'latest_sensor2': Gauge('fms_latest_sensor2', 'Latest value of sensor2 for a device', ['device_id']),
    'latest_sensor3': Gauge('fms_latest_sensor3', 'Latest value of sensor3 for a device', ['device_id']),
    'latest_motor1': Gauge('fms_latest_motor1', 'Latest value of motor1 for a device', ['device_id']),
    'latest_motor2': Gauge('fms_latest_motor2', 'Latest value of motor2 for a device', ['device_id']),
    'latest_motor3': Gauge('fms_latest_motor3', 'Latest value of motor3 for a device', ['device_id']),
}

def update_latest_metrics(spark):
    """S3ì—ì„œ ë°ì´í„°ë¥¼ ì½ê³ , ì¥ë¹„ë³„ ìµœì‹  ê°’ì„ ì°¾ì•„ í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ ìµœì‹  ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤... (ê²½ë¡œ: {S3_DATA_PATH})")
    
    try:
        # 1. S3ì—ì„œ Parquet ë°ì´í„° ì½ê¸°
        df = spark.read.parquet(S3_DATA_PATH)

        # 2. ê° DeviceId ë‚´ì—ì„œ time ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ë ˆì½”ë“œ ì°¾ê¸°
        window_spec = Window.partitionBy("DeviceId").orderBy(col("time").desc())
        df_latest = df.withColumn("rank", row_number().over(window_spec)) \
                      .filter(col("rank") == 1) \
                      .drop("rank")
        
        df_recent = df_latest.filter(
            (col("ts") >= current_timestamp() - expr("INTERVAL 5 MINUTE"))
        )

        # 3. ì°¾ì€ ìµœì‹  ê°’ì„ í”„ë¡œë©”í…Œìš°ìŠ¤ ê²Œì´ì§€ì— ë°˜ì˜
        for row in df_recent.collect():
            device_id = str(row["DeviceId"])
            for metric_name, gauge in LATEST_METRICS.items():
                # 'latest_' ì ‘ë‘ì‚¬ë¥¼ ì œì™¸í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ê°’ì„ ê°€ì ¸ì˜´
                col_name = metric_name.replace('latest_', '')
                if row[col_name] is not None:
                    gauge.labels(device_id=device_id).set(row[col_name])
        
        print(f"âœ… {df_recent.count()}ê°œ ì¥ë¹„ì˜ ìµœì‹  ë©”íŠ¸ë¦­ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜: Spark ì„¸ì…˜ ìƒì„±, í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ ì‹œì‘ ë° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ë£¨í”„ ì‹¤í–‰"""
    # Spark ì„¸ì…˜ ìƒì„±
    spark = SparkSession.builder \
        .appName("FMS S3 Latest to Prometheus") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
        .getOrCreate()

    # Spark ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    spark.sparkContext.setLogLevel("ERROR")

    print(f"ğŸš€ í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (í¬íŠ¸: {PROMETHEUS_PORT})")
    start_http_server(PROMETHEUS_PORT)
    print(f"ğŸ”— ë©”íŠ¸ë¦­ í™•ì¸: http://<your-spark-driver-ip>:{PROMETHEUS_PORT}")

    try:
        while True:
            update_latest_metrics(spark)
            print(f"ğŸ•’ ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ {REFRESH_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            time.sleep(REFRESH_INTERVAL_SECONDS)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()

