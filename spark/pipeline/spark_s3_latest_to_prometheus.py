

import time
from datetime import datetime, timedelta
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, from_json, row_number, lit, to_timestamp, date_format
from prometheus_client import start_http_server, Gauge

# --- ì„¤ì • ---
PROMETHEUS_PORT = 9990  # ì´ì „ ìŠ¤í¬ë¦½íŠ¸ì™€ ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
REFRESH_INTERVAL_SECONDS = 30  # 30ì´ˆë§ˆë‹¤ ë°ì´í„° ê°±ì‹ 
DATA_RETENTION_MINUTES = 60  # 60ë¶„ ì´ë‚´ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©

# --- í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì •ì˜ ---
# 'device_id'ë¥¼ ë¼ë²¨ë¡œ ì‚¬ìš©í•˜ì—¬ ì¥ë¹„ë³„ ìµœì‹  ê°’ì„ êµ¬ë¶„
LATEST_METRICS = {
    'sensor1': Gauge('fms_latest_sensor1', 'Latest value of sensor1 for a device', ['device_id']),
    'sensor2': Gauge('fms_latest_sensor2', 'Latest value of sensor2 for a device', ['device_id']),
    'sensor3': Gauge('fms_latest_sensor3', 'Latest value of sensor3 for a device', ['device_id']),
    'motor1': Gauge('fms_latest_motor1', 'Latest value of motor1 for a device', ['device_id']),
    'motor2': Gauge('fms_latest_motor2', 'Latest value of motor2 for a device', ['device_id']),
    'motor3': Gauge('fms_latest_motor3', 'Latest value of motor3 for a device', ['device_id']),
}

def update_latest_metrics(spark):
    now = datetime.now()
    partition_path = f"s3a://awsprelab1/fms/analytics_parquet/data/{now.year:04d}/{now.month:02d}/{now.day:02d}/{now.hour:02d}/*"
    """S3ì˜ JSON ë°ì´í„°ë¥¼ ì½ê³ , ì¥ë¹„ë³„ ìµœì‹  ê°’ì„ ì°¾ì•„ í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ ìµœì‹  ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤... (ê²½ë¡œ: {partition_path})")
    
    try:
        # 1. S3ì—ì„œ Parquet ë°ì´í„° ì½ê¸°
        df = spark.read.parquet(partition_path)

        # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± ë° 60ë¶„ ì´ë‚´ ë°ì´í„° í•„í„°ë§
        df_with_ts = df.withColumn("ts", to_timestamp(col("collected_at")))
        time_threshold = datetime.now() - timedelta(minutes=DATA_RETENTION_MINUTES)
        df_recent = df_with_ts.filter(col("ts") >= lit(time_threshold).cast("timestamp"))

        if df_recent.rdd.isEmpty():
            print("âœ… ì²˜ë¦¬í•  ìµœì‹  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for gauge in LATEST_METRICS.values():
                gauge.clear()
            return

        # 3. ê° DeviceId ë‚´ì—ì„œ time ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ë ˆì½”ë“œ ì°¾ê¸°
        window_spec = Window.partitionBy("DeviceId").orderBy(col("time").desc())
        df_latest = df_recent.withColumn("rank", row_number().over(window_spec)) \
                             .filter(col("rank") == 1) \
                             .select("DeviceId", "sensor1", "sensor2", "sensor3", "motor1", "motor2", "motor3")
        
        # 4. ì°¾ì€ ìµœì‹  ê°’ì„ í”„ë¡œë©”í…Œìš°ìŠ¤ ê²Œì´ì§€ì— ë°˜ì˜
        for gauge in LATEST_METRICS.values():
            gauge.clear()

        for row in df_latest.collect():
            labels = {"device_id": str(row["DeviceId"])}
            for metric_name, gauge in LATEST_METRICS.items():
                value = row[metric_name]
                if value is not None:
                    gauge.labels(**labels).set(float(value))
        
        print(f"âœ… {df_latest.count()}ê°œ ì¥ë¹„ì˜ ìµœì‹  ë©”íŠ¸ë¦­ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜: Spark ì„¸ì…˜ ìƒì„±, í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ ì‹œì‘ ë° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ë£¨í”„ ì‹¤í–‰"""
    # Spark ì„¸ì…˜ ìƒì„±
    spark = SparkSession.builder \
        .appName("FMS S3 Latest to Prometheus") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
        .getOrCreate()

    # Spark ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    spark.sparkContext.setLogLevel("ERROR")

    print(f"ğŸš€ í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (í¬íŠ¸: {PROMETHEUS_PORT})")
    start_http_server(PROMETHEUS_PORT)
    print(f"ğŸ”— ë©”íŠ¸ë¦­ í™•ì¸: http://<your-spark-driver-ip>:{PROMETHEUS_PORT}")

    try:
        while True:
            update_latest_metrics(spark)
            print(f"ğŸ•’ ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ {REFRESH_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            time.sleep(REFRESH_INTERVAL_SECONDS)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()

