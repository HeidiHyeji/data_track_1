#!/usr/bin/env python3
"""
FMS Consumer for Real Data Processing
Kafkaë¡œë¶€í„° FMS ì„¼ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³ , 30ì´ˆë§ˆë‹¤ íŒŒì¼ì„ ì €ì¥í•˜ì—¬ S3ì— ì—…ë¡œë“œ (íŒŒì¼ëª…ì€ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
"""
import json
import os
import tempfile
import subprocess
import time
from datetime import datetime
from confluent_kafka import Consumer, KafkaError
import logging

# Kafka ì„¤ì •
BROKER = "s1:9092,s2:9092,s3:9092,i1:9092"
TOPIC = "fms-sensor-data"
GROUP_ID = "fms-data-processor"

# S3 ê²½ë¡œ ì„¤ì •
S3_BUCKET = "awsprelab1"
S3_PREFIX = "fms/raw-data/"
S3_URI = f"s3://{S3_BUCKET}/{S3_PREFIX}"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FMSDataConsumer:
    def __init__(self):
        self.consumer = Consumer({
            'bootstrap.servers': BROKER,
            'group.id': GROUP_ID,
            'auto.offset.reset': 'earliest'
        })
        self.buffer = []
        self.last_flush_time = time.time()

    def write_buffer_to_s3(self):
        """ë²„í¼ ë‚´ìš©ì„ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ë¡œ S3ì— ì €ì¥"""
        if not self.buffer:
            return

        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sensor-data-{timestamp_str}.json"
        s3_path = os.path.join(S3_URI, filename)

        try:
            with tempfile.NamedTemporaryFile('w', delete=False) as tmp:
                for record in self.buffer:
                    tmp.write(json.dumps(record) + '\n')
                tmp_path = tmp.name

            subprocess.run(
                ["aws", "s3", "cp", tmp_path, s3_path],
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            logger.info(f"ğŸ“¤ S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_path}")
        except subprocess.CalledProcessError as e:
            logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e.stderr.strip()}")
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass
            self.buffer.clear()
            self.last_flush_time = time.time()

    def process_message(self, msg):
        try:
            raw_value = msg.value().decode('utf-8')
            logger.info(f"[RAW INPUT] {raw_value}")
            data = json.loads(raw_value)
            self.buffer.append(data)
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def run(self):
        logger.info("FMS Data Consumer ì‹œì‘... (S3 ì—…ë¡œë“œ ëª¨ë“œ)")
        logger.info(f"êµ¬ë… í† í”½: {TOPIC}")

        self.consumer.subscribe([TOPIC])

        try:
            while True:
                msg = self.consumer.poll(1.0)

                if msg is None:
                    continue
                elif msg.error():
                    if msg.error().code() != KafkaError._PARTITION_EOF:
                        logger.error(f"Consumer error: {msg.error()}")
                else:
                    self.process_message(msg)

                if time.time() - self.last_flush_time >= 30:
                    self.write_buffer_to_s3()

        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        finally:
            self.write_buffer_to_s3()
            self.consumer.close()
            logger.info("Consumer ì¢…ë£Œ")

if __name__ == "__main__":
    consumer = FMSDataConsumer()
    consumer.run()
