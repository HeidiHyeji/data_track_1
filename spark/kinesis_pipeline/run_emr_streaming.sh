#!/bin/bash

# === [1] ë³€ìˆ˜ ì„¤ì • ===
LOCAL_FILE="spark_preprocessing_streaming.py"
S3_BUCKET="awsprelab1"
S3_KEY="scripts/spark_preprocessing_streaming.py"
S3_URI="s3://${S3_BUCKET}/${S3_KEY}"
PARTITION_SCRIPT="add_all_partitions_to_athena.py"
PARTITION_S3_KEY="scripts/${PARTITION_SCRIPT}"
PARTITION_S3_URI="s3://${S3_BUCKET}/${PARTITION_S3_KEY}"

# === [2] EMR í´ëŸ¬ìŠ¤í„° ID ìˆ˜ë™ ì…ë ¥ ë°›ê¸° ===
echo "ğŸ“ EMR í´ëŸ¬ìŠ¤í„° IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: j-XXXXXXXXXXXXX):"
read -r CLUSTER_ID

if [ -z "$CLUSTER_ID" ]; then
  echo "âŒ í´ëŸ¬ìŠ¤í„° IDê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤."
  exit 1
fi

echo "âœ… ì…ë ¥ëœ í´ëŸ¬ìŠ¤í„° ID: $CLUSTER_ID"

# === [3] S3 ì—…ë¡œë“œ ===
echo "ğŸ“¤ S3ë¡œ ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ ì¤‘: $LOCAL_FILE â†’ $S3_URI"
aws s3 cp "$LOCAL_FILE" "$S3_URI"

if [ $? -ne 0 ]; then
  echo "âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨"
  exit 1
fi

echo "ğŸ“¤ S3ë¡œ íŒŒí‹°ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ ì¤‘: $PARTITION_SCRIPT â†’ $PARTITION_S3_URI"
aws s3 cp "$PARTITION_SCRIPT" "$PARTITION_S3_URI"

if [ $? -ne 0 ]; then
  echo "âŒ íŒŒí‹°ì…˜ ìŠ¤í¬ë¦½íŠ¸ S3 ì—…ë¡œë“œ ì‹¤íŒ¨"
  exit 1
fi

# === [4] EMR Spark Step ë“±ë¡ ===
echo "ğŸš€ EMR Spark Step ë“±ë¡ ì¤‘..."

aws emr add-steps \
  --cluster-id "$CLUSTER_ID" \
  --steps Type=Spark,Name="FMS_Streaming",ActionOnFailure=CONTINUE,\
Args=[--deploy-mode,cluster,--master,yarn,\
--packages,org.apache.hadoop:hadoop-aws:3.3.4,\
${S3_URI}]

if [ $? -eq 0 ]; then
  echo "âœ… Spark ìŠ¤íŠ¸ë¦¬ë° Step ë“±ë¡ ì™„ë£Œ"
else
  echo "âŒ Spark Step ë“±ë¡ ì‹¤íŒ¨"
  exit 1
fi

# === [5] Athena íŒŒí‹°ì…˜ ìë™ ë“±ë¡ Step ===
echo "ğŸš€ Athena íŒŒí‹°ì…˜ ë“±ë¡ Step ì¶”ê°€ ì¤‘..."

aws emr add-steps \
  --cluster-id "$CLUSTER_ID" \
  --steps Type=CUSTOM_JAR,Name="AddAllAthenaPartitions",ActionOnFailure=CONTINUE,\
Jar=command-runner.jar,Args=["bash","-c","aws s3 cp ${PARTITION_S3_URI} /home/hadoop/${PARTITION_SCRIPT} && python3 /home/hadoop/${PARTITION_SCRIPT}"]

if [ $? -eq 0 ]; then
  echo "âœ… Athena íŒŒí‹°ì…˜ Step ë“±ë¡ ì™„ë£Œ"
else
  echo "âŒ Athena íŒŒí‹°ì…˜ Step ë“±ë¡ ì‹¤íŒ¨"
  exit 1
fi
