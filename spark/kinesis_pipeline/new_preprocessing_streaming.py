from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth, hour, from_utc_timestamp
from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, BooleanType

# Spark ì„¸ì…˜ ìƒì„±
spark = SparkSession.builder \
    .appName("FMS_Custom_S3_Path") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
    .getOrCreate()

# ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType() \
    .add("time", StringType()) \
    .add("DeviceId", IntegerType()) \
    .add("sensor1", DoubleType()) \
    .add("sensor2", DoubleType()) \
    .add("sensor3", DoubleType()) \
    .add("motor1", DoubleType()) \
    .add("motor2", DoubleType()) \
    .add("motor3", DoubleType()) \
    .add("isFail", BooleanType()) \
    .add("collected_at", StringType())

# ìŠ¤íŠ¸ë¦¼ ì…ë ¥
df_raw = spark.readStream \
    .format("json") \
    .schema(schema) \
    .option("maxFilesPerTrigger", 1) \
    .load("s3a://awsprelab1/fms/raw-data/*/*/*/*/*.json")

# ì‹œê°„ ë° íŒŒí‹°ì…˜ìš© ì»¬ëŸ¼ ìƒì„± (KST ê¸°ì¤€)
df = df_raw \
    .withColumn("ts_utc", to_timestamp(col("time"))) \
    .withColumn("ts", to_timestamp(col("time"))) \
    .withColumn("year", year(col("ts"))) \
    .withColumn("month", month(col("ts"))) \
    .withColumn("day", dayofmonth(col("ts"))) \
    .withColumn("hour", hour(col("ts")))

# ìœ íš¨ì„± ì¡°ê±´ ì •ì˜
valid_range = (
    (col("sensor1").between(0, 100)) &
    (col("sensor2").between(0, 100)) &
    (col("sensor3").between(0, 150)) &
    (col("motor1").between(0, 2000)) &
    (col("motor2").between(0, 1500)) &
    (col("motor3").between(0, 1800)) &
    (col("DeviceId").between(1, 100)) &
    (col("isFail").isin(True, False))
)

# ë°ì´í„° ë¶„ê¸°
df_correct = df.filter((col("isFail") == False) & valid_range)
df_dataerr = df.filter((col("isFail") == False) & (~valid_range))
df_fail = df.filter(col("isFail") == True)
# ë°°ì¹˜ ì €ì¥ í•¨ìˆ˜: ë””ë ‰í† ë¦¬ ì§ì ‘ êµ¬ì„±
def write_batch_to_custom_path(batch_df, batch_id, category):
    batch_df_local = batch_df.withColumn("YYYY", col("year")) \
                             .withColumn("MM", col("month")) \
                             .withColumn("DD", col("day")) \
                             .withColumn("HH", col("hour")) \
                             .withColumn("ID", col("DeviceId"))

    partitions = batch_df_local.select("YYYY", "MM", "DD", "HH", "ID").distinct().collect()

    for row in partitions:
        yyyy = int(row["YYYY"])
        mm = int(row["MM"])
        dd = int(row["DD"])
        hh = int(row["HH"])
        device_id = int(row["ID"])

        path = f"s3a://awsprelab1/fms/analytics_parquet/{category}/{yyyy:04d}/{mm:02d}/{dd:02d}/{hh:02d}/{device_id:02d}/"

        print(f"ğŸ“¦ [{category}] Writing to: {path}")

        batch_df_local \
            .filter(
                (col("year") == yyyy) &
                (col("month") == mm) &
                (col("day") == dd) &
                (col("hour") == hh) &
                (col("DeviceId") == device_id)
            ) \
            .drop("YYYY", "MM", "DD", "HH", "ID") \
            .write.mode("append").parquet(path)


# ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
query_data = df_correct.writeStream \
    .foreachBatch(lambda df, id: write_batch_to_custom_path(df, id, "data")) \
    .outputMode("append") \
    .option("checkpointLocation", "s3a://awsprelab1/fms/analytics_parquet/data_checkpoint") \
    .start()

query_dataerr = df_dataerr.writeStream \
    .foreachBatch(lambda df, id: write_batch_to_custom_path(df, id, "dataerr")) \
    .outputMode("append") \
    .option("checkpointLocation", "s3a://awsprelab1/fms/analytics_parquet/dataerr_checkpoint") \
    .start()

query_fail = df_fail.writeStream \
    .foreachBatch(lambda df, id: write_batch_to_custom_path(df, id, "fail")) \
    .outputMode("append") \
    .option("checkpointLocation", "s3a://awsprelab1/fms/analytics_parquet/fail_checkpoint") \
    .start()

# ì‹¤í–‰ ëŒ€ê¸°
query_data.awaitTermination()
query_dataerr.awaitTermination()
query_fail.awaitTermination()
